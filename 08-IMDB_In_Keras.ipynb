{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Analyzing IMDB Data in Keras", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "# Imports\nimport numpy as np\nimport keras\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(42)"
        }, 
        {
            "source": "## 1. Loading the data\nThis dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "# of Training Samples: 25000\n# of Test Samples: 25000\n# of Classes: 2\n"
                }
            ], 
            "source": "# Loading the data (it's preloaded in Keras)\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=None)\n\n#print(x_train.shape)\n#print(x_test.shape)\nprint('# of Training Samples: {}'.format(len(x_train)))\nprint('# of Test Samples: {}'.format(len(x_test)))\n\nnum_classes = max(y_train) + 1\nprint('# of Classes: {}'.format(num_classes))"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "word_index = imdb.get_word_index(path=\"imdb_word_index.json\")"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "index of the is  14\nthe as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n1\n"
                }
            ], 
            "source": "## so, word index is a key value pair... key is word and value is its frequency\n## so we can find index of word 'at' by using syntax word_index['at']... and the result is 25.. \n## therefore, at is 25th most frequently occuring word.\n##print(\"word index is - \", word_index)\n\nprint(\"index of the is \", word_index['as'])\n## index to word dictionary will be opposite, it will have frequency as an 'index' and value as 'word'\nindex_to_word = {}\nfor key, value in word_index.items():\n    ## A new dictionary will be created.\n    index_to_word[value] = key\n    \n## below print will print the reuter news article at index 0.     \nprint(' '.join([index_to_word[x] for x in x_train[0]]))\n\nprint(y_train[0])\n\n#[index_to_word[x] for x in x_train[0]]\n\n"
        }, 
        {
            "source": "## 2. Examining the data\nNotice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n\nThe output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n1\n"
                }
            ], 
            "source": "print(x_train[0])\nprint(y_train[0])"
        }, 
        {
            "source": "## 3. One-hot encoding the output\nHere, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[ 0.  1.  0. ...,  0.  0.  0.]\n"
                }
            ], 
            "source": "# One-hot encoding the output into vector mode, each of length 1000\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\nprint(x_train[0])"
        }, 
        {
            "source": "And we'll also one-hot encode the output.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(25000, 2)\n(25000, 2)\n"
                }
            ], 
            "source": "# One-hot encoding the output\nnum_classes = 2\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\nprint(y_train.shape)\nprint(y_test.shape)"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10000\n1.0\n[ 0.  1.]\n2\n"
                }
            ], 
            "source": "#print(x_train[0])\nprint(len(x_train[0]))\nprint(max(x_train[0]))\n\nprint(y_train[0])\nprint(len(y_train[0]))"
        }, 
        {
            "source": "## 4. Building the  model architecture\nBuild a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['loss', 'acc']\n"
                }
            ], 
            "source": "# TODO: Build the model architecture\n\nmodel = Sequential()\n## here features are words ...so, the input array that we will feed to the network is max words\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\n## 50 percent of the nodes from the hidden layer are removed randomly.\n## this is a good regularization to avoid overfitting\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n\n\n\n\n# TODO: Compile the model using a loss function and an optimizer.\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.metrics_names)\n"
        }, 
        {
            "source": "## 5. Training the model\nRun the model here. Experiment with different batch_size, and number of epochs!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 22500 samples, validate on 2500 samples\nEpoch 1/5\n22500/22500 [==============================] - 44s 2ms/step - loss: 0.2778 - acc: 0.8855 - val_loss: 0.2734 - val_acc: 0.8940\nEpoch 2/5\n22500/22500 [==============================] - 45s 2ms/step - loss: 0.1551 - acc: 0.9419 - val_loss: 0.2988 - val_acc: 0.8932\nEpoch 3/5\n22500/22500 [==============================] - 43s 2ms/step - loss: 0.0777 - acc: 0.9731 - val_loss: 0.3622 - val_acc: 0.8836\nEpoch 4/5\n22500/22500 [==============================] - 44s 2ms/step - loss: 0.0319 - acc: 0.9905 - val_loss: 0.4366 - val_acc: 0.8852\nEpoch 5/5\n22500/22500 [==============================] - 44s 2ms/step - loss: 0.0120 - acc: 0.9968 - val_loss: 0.5035 - val_acc: 0.8804\n"
                }
            ], 
            "source": "# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\n\nbatch_size = 32\nepochs = 5\n\nhistory = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n"
        }, 
        {
            "source": "## 6. Evaluating the model\nThis will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25000/25000 [==============================] - 8s 321us/step\nTest loss: 0.52223496568\nTest accuracy: 0.8708\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 512)               5120512   \n_________________________________________________________________\nactivation_1 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 1026      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 5,121,538\nTrainable params: 5,121,538\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ], 
            "source": "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nmodel.summary()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}